{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: smaller; color: #808080; text-align: center; display: block;\">MIT - 6.802 / 6.874 / 20.390 / 20.490 / HST.506 Computational Systems Biology: Deep Learning in the Life Sciences - Spring 2019</div>\n",
    "\n",
    "# Problem Set 2: Transcription factor binding site prediction with CNNs (and RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set we are going to train a CNN to classify the binding sequences of the transcription factor CTCF. The positive examples are 101 bp DNA sequences centered at CTCF ChIP-seq peaks from GM12878 cells. Negative sequences are generated by permuting the nucleotides in the positive sequences while keeping the di-nucleotide frequency. All the sequences have been embedded using one-hot encoding. Unlike problem set 1, which was a multi-class classification task (10 digits), problem set 2 is a binary classification task (CTCF is binding, CTCF is not binding).\n",
    "\n",
    "Concepts and techniques relevant to this problem set:\n",
    "- convolutional layers\n",
    "- recurrent layers (for 6.874 students only)\n",
    "- model evaluation with ROC and precision-recall curves\n",
    "- model serialization and deserialization (storing models on disk)\n",
    "- grid search as hyperparameter optimization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f521f4bedf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import libraries, set random seed for reproducibility, and print TensorFlow version\n",
    "\n",
    "# if libraries are missing:\n",
    "# !pip install tensorflow\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install h5py\n",
    "# !pip install sklearn\n",
    "# !pip install tqdm\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import h5py\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "random.seed(0)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the data embedded in the previous problem and their labels \n",
    "def load_data(prefix='', path='./data/'):\n",
    "    training_set = h5py.File(os.path.join(path, prefix + 'train.h5'), 'r')\n",
    "    validation_set = h5py.File(os.path.join(path, prefix + 'valid.h5'), 'r')\n",
    "    test_set = h5py.File(os.path.join(path, prefix + 'test.h5'), 'r')\n",
    "    \n",
    "    return(training_set['data'][:], training_set['label'][:],\n",
    "           validation_set['data'][:], validation_set['label'][:],\n",
    "           test_set['data'][:], test_set['label'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "SEQ_LEN = 101\n",
    "NUM_BASES = 4\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "x_tr, y_tr, x_va, y_va, x_te, y_te = load_data()\n",
    "print(\"training set:\", x_tr.shape)\n",
    "print(\"validation set:\", x_va.shape)\n",
    "print(\"test set:\", x_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Construct a CNN with given specifications (50 points)\n",
    "\n",
    "Implement a CNN model with the following architecture: \n",
    "1. one convolutional layer with 32 kernels (filters) of width 11 and height 1, step size of 1 and ReLU activation. The input should be padded so that the output has the same size as the original input.\n",
    "2. one max-pooling layer with pooling size of 2\n",
    "3. flatten the tensor to prepare it for the fully connected layer\n",
    "4. one fully connected layer with 64 neurons and ReLU activation\n",
    "5. one dropout layer with a `dropout_rate` (dropout rate = 1 - keep probability)\n",
    "6. one fully connected layer with 2 neurons and sigmoid activation\n",
    "7. all the weights (not the biases) in the network should be returned in list `weights` (for L2 regularization in function `cnn_loss`)\n",
    "\n",
    "About weight initialization:\n",
    "- do not initialize weights with zeros or other constant values!\n",
    "- use random initialization to [break symmetries](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94), e.g.\n",
    "  - zero-centered Gaussian with low variance (e.g., $\\sigma = 0.01$)\n",
    "  - zero-centered truncated Gaussian with low variance (e.g., $\\sigma = 0.01$)\n",
    "  - Xavier intialization\n",
    "  - He initialization\n",
    "\n",
    "About bias initialization:\n",
    "- can be initialized to constant value, also zero (since weight initialization breaks symmetry)\n",
    "- commonly initialized to 0.1, 0.01, 0.0, 1.0\n",
    "\n",
    "Useful functions:\n",
    "- [tf.truncated_normal](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)\n",
    "- [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "- [tf.nn.bias_add](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add)\n",
    "- [tf.nn.relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "- [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "- [tf.nn.dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
    "- [tf.nn.softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)\n",
    "- [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x_ph, dropout_rate):\n",
    "    ################################################################################ \n",
    "    # Implement a CNN with the required structure and initialize all the variables:\n",
    "    # Arguments:\n",
    "    # - x_ph: one-hot encoded DNA input data / placeholder of shape [?, 1, 101, 4]\n",
    "    # - dropout_rate: dropout rate (1 - keep probability) of dropout layer\n",
    "    # Return values:\n",
    "    # - y_hat_op: class probabilities operator (our predicted y, sigmoid(z_op)) of shape [?, 2]\n",
    "    # - z_op: unscaled log probabilities operator (output of last matrix multiplication,\n",
    "    #      before activation function) of shape [?, 2]\n",
    "    # - weights: a list of all tf.Variable weight matrices\n",
    "    ################################################################################\n",
    "    # x_ph: [batch, in_height, in_width, in_channels] -> [?, 1, 101, 4]\n",
    "    # conv layer filter: [filter_height, filter_width, in_channels, out_channels] -> [1, 11, 4, 32]\n",
    "    conv_filter_height = 1\n",
    "    conv_filter_width = 11\n",
    "    in_channels = 4\n",
    "    out_channels = 32\n",
    "    fc_units = 64\n",
    "    max_pool_stride = 2\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return y_hat_op, z_op, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions:\n",
    "- [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)\n",
    "- [tf.nn.l2_loss](https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss)\n",
    "- [tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_loss(z_op, y_ph, weights, l2_coefficient):\n",
    "    ################################################################################ \n",
    "    # Implement a loss operator, which calculates an L2 regularized cross entropy \n",
    "    # loss \n",
    "    # Arguments:\n",
    "    # - z_op: unscaled log probabilities (output of last layer) of shape [?, 2]\n",
    "    # - y_ph: labels / placeholder of shape [?, 2]\n",
    "    # - weights: list of all weight tf.Variables\n",
    "    # - l2_coefficient: coefficient lambda for L2 regularization of weights\n",
    "    # Return values:\n",
    "    # - loss_op: the loss operator\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return loss_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Implement the training loop (30 points)\n",
    "\n",
    "In order to perform hyper-parameter search later, we need a training function that takes the hyperparameter configuration as input parameter.\n",
    "\n",
    "Implement `training`:\n",
    "- initialize all variables\n",
    "- train for `num_epochs` epochs. In each epoch, train on mini-batch of size `batch_size`.\n",
    "- at the end of each epoch:\n",
    "  - calculate validation loss\n",
    "  - save model if validation loss is lower than validation loss of previous epochs (inputs: `x_ph`, `y_ph`; output: `y_hat_op`, `loss_op`)\n",
    "\n",
    "Useful functions:\n",
    "- [tf.saved_model.simple_save](https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save)\n",
    "\n",
    "[How to save a model with simple_save](https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model/50852627#50852627). Don't forget to name operators that you what to restore later (e.g., `y_hat_op`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x_train, y_train, x_val, y_val, hyperparam_config, num_epochs, batch_size,\n",
    "             save_model=True, model_dir='models/best_model'): \n",
    "    ################################################################################\n",
    "    # Arguments:\n",
    "    # - x_train: input training set\n",
    "    # - y_train: label training set\n",
    "    # - x_val: input validation set\n",
    "    # - y_val: label validation set\n",
    "    # - hyperparam_config: a dictionary that stores a hyperparameter configuration,\n",
    "    #                      including:\n",
    "    #                      - \"dropout_rate\": dropout rate (1 - keep probability),\n",
    "    #                      - \"l2\": coefficient lambda for L2 regularization,\n",
    "    #                      - \"lr\": learning rate for RMSProp optimizer\n",
    "    # - num_epochs: number of epochs to train\n",
    "    # - batch_size: training mini-batch size\n",
    "    # - best_model_dir: location where model will be saved\n",
    "    # Return values:\n",
    "    # - best_loss: best validation loss\n",
    "    ################################################################################\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # define input and output placeholders\n",
    "    # TF uses NHWC: the data is stored in the order of [batch, in_height, in_width, in_channels]\n",
    "    # DNA is one-dimensional (height = 1, width = 101) with four channels for the four bases A, C, G, T\n",
    "    x_ph = tf.placeholder(\"float\", [None, 1, SEQ_LEN, NUM_BASES], name=\"x_ph\")\n",
    "    y_ph = tf.placeholder(\"float\", [None, NUM_CLASSES], name=\"y_ph\")\n",
    "\n",
    "    # model\n",
    "    y_hat_op, z_op, weights = cnn_model(x_ph, hyperparam_config['dropout_rate'])\n",
    "    \n",
    "    # loss\n",
    "    loss_op = cnn_loss(z_op, y_ph, weights, hyperparam_config['l2'])\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_op = tf.train.RMSPropOptimizer(hyperparam_config['lr'], 0.9).minimize(loss_op)\n",
    "     \n",
    "    # Implement training loop\n",
    "    # - loop through training data num_epochs times\n",
    "    # - after each epoch, calculate loss on validation set (x_val, y_val)\n",
    "    # - if save_model: save model with lowest validation loss to disk (model_dir)\n",
    "    # - (delete previous best model)\n",
    "    # - return lowest validation loss\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Hyperparameter search (10 points)\n",
    "\n",
    "In the following we will call the `training` function multiple times with different hyperparameter configurations. We first define a handful of possible values for each hyperparameter and then test all combinations. This most basic version of hyperparameter optimization is called grid search.\n",
    "\n",
    "To save time, we will only train on a subsample of the training set (`num_training`) and evaluate the loss on a subsample of the validation set (`num_validation`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_set(x, y, n):\n",
    "    ################################################################################ \n",
    "    # Draws n examples without replacement from sets x, y\n",
    "    # Arguments:\n",
    "    # - x: input set\n",
    "    # - y: label set\n",
    "    # - n: number of examples drawn\n",
    "    # Return values:\n",
    "    # - x_subset: randomly selected x\n",
    "    # - y_subset: corresponding labels of randomly selected x\n",
    "    ################################################################################\n",
    "    idx = np.arange(len(x))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[0 : n]\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(x_train, y_train, x_val, y_val, dropout_rates, l2_lambdas, learning_rates,\n",
    "                num_training, num_validation, num_epochs=5, batch_size=128):\n",
    "    ################################################################################ \n",
    "    # Arguments:\n",
    "    # - x_train: input training set\n",
    "    # - y_train: label training set\n",
    "    # - x_val: input validation set\n",
    "    # - y_val: label validation set\n",
    "    # - dropout_rates: dropout rates to try\n",
    "    # - l2_lambdas: L2 lambda coefficients to try\n",
    "    # - learning_rates: learning rates to try\n",
    "    # - num_training: number of training examples used per model\n",
    "    # - num_validation: number of validation examples used per model\n",
    "    # - num_epochs: number of epochs to train\n",
    "    # - batch_size: training mini-batch size\n",
    "    # Return values:\n",
    "    # - losses: losses for configurations tested\n",
    "    ################################################################################\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(total = len(dropout_rates) * len(l2_lambdas) * len(learning_rates))\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for l2_lambda in l2_lambdas:\n",
    "            for learning_rate in learning_rates:\n",
    "                # subsample training and validation sets\n",
    "                subset_x_train, subset_y_train = subsample_set(x_train, y_train, num_training)\n",
    "                subset_x_val, subset_y_val = subsample_set(x_val, y_val, num_validation)\n",
    "                \n",
    "                hyperparam_config = {'dropout_rate': dropout_rate,\n",
    "                                     'l2': l2_lambda,\n",
    "                                     'lr': learning_rate}\n",
    "                \n",
    "                best_loss = training(subset_x_train, subset_y_train,\n",
    "                                     subset_x_val, subset_y_val,\n",
    "                                     hyperparam_config,\n",
    "                                     num_epochs,\n",
    "                                     batch_size,\n",
    "                                     save_model = False)\n",
    "                \n",
    "                losses.append([dropout_rate, l2_lambda, learning_rate, best_loss])\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter values to test\n",
    "dropout_rates = [1.0 - 1e-3, 0.8, 0.5, 0.2]\n",
    "l2_lambdas = [1e-03, 1e-06, 0]\n",
    "learning_rates = [1e-1, 1e-3, 1e-5]\n",
    "\n",
    "# static hyperparameters\n",
    "num_training = 10000\n",
    "num_validation = 2000\n",
    "batch_size = 128\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_result = grid_search(x_tr, y_tr, x_va, y_va, dropout_rates, l2_lambdas, learning_rates,\n",
    "                           num_training, num_validation, num_epochs, batch_size)\n",
    "df = pd.DataFrame(param_result,\n",
    "                  columns = ['dropout rate', \n",
    "                             'L2 lambda',\n",
    "                             'learning rate',\n",
    "                             'validation loss']).pivot_table(values = ['validation loss'],\n",
    "                                                             columns = ['L2 lambda'],\n",
    "                                                             index = ['dropout rate', 'learning rate'])\n",
    "df = df.round(3)\n",
    "(df.style.background_gradient(cmap=sb.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "#### Question 3.1\n",
    "\n",
    "Based on your observation, is the performance impacted by only one hyper-parameter or several? Describe your observation to support your conclusion.\n",
    "\n",
    "<span style=\"text-transform: uppercase; color: #408080; font-size: bigger; font-weight: bold;\">Your (short) answer:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2\n",
    "\n",
    "What have you observed when the L2 coefficient is 1e-03? What do you think will happen if we have an even larger coefficient, and why?\n",
    "\n",
    "<span style=\"text-transform: uppercase; color: #408080; font-size: bigger; font-weight: bold;\">Your (short) answer:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.3\n",
    "\n",
    "- Assuming each hyper-parameter has a constant number of choices to pick from, how fast does the search space grow as a function of the number of hyper-parameters to consider?\n",
    "- Can you think of smarter ways than grid search to explore the hyperparameter space?\n",
    "\n",
    "<span style=\"text-transform: uppercase; color: #408080; font-size: bigger; font-weight: bold;\">Your (short) answer:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Evaluate best model on test set (10 points)\n",
    "\n",
    "Now let's train the network with the best hyper-parameters on the **whole** training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = \"models/best_model\"\n",
    "################################################################################\n",
    "#                            BEGINNING OF YOUR CODE                            #\n",
    "################################################################################\n",
    "# change dropout rate, L2 coefficient, and learning rate to\n",
    "# best values (according to grid search above):\n",
    "hyperparam_config = {'dropout_rate': 0,\n",
    "                     'l2': 0,\n",
    "                     'lr': 0}\n",
    "################################################################################\n",
    "#                               END OF YOUR CODE                               #\n",
    "################################################################################\n",
    "validation_loss = training(x_tr, y_tr, x_va, y_va,\n",
    "                           hyperparam_config, num_epochs, batch_size, save_model=True, model_dir=best_model_dir)\n",
    "print('validation loss:', validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `predict` function. Restoring a previously saved model is described [here](https://www.tensorflow.org/guide/saved_model) and [here](https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model/50852627#50852627).\n",
    "\n",
    "Useful functions:\n",
    "- [tf.saved_model.load](https://www.tensorflow.org/api_docs/python/tf/saved_model/load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model_dir):\n",
    "    ################################################################################\n",
    "    # load a trained model and predict y_hat for examples in x\n",
    "    # Arguments\n",
    "    # - x: one-hot encoded DNA input data of shape [?, 1, 101, 4]\n",
    "    # - model_dir: where the model was saved\n",
    "    # Return values: \n",
    "    # - y_hat: class probabilities of examples in x\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evalutation\n",
    "\n",
    "We will evaluate the trained model using the `predict` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = predict(x_te, best_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve\n",
    "\n",
    "The area under the ROC curve (auROC) evaluated on the test set should be around 0.94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y, y_hat):\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(y[:, 0], y_hat[:, 0])\n",
    "    roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_te, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(y, y_hat):\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(y[:, 0], y_hat[:, 0])\n",
    "    pr_auc = sklearn.metrics.auc(recall, precision)\n",
    "    f1 = sklearn.metrics.f1_score(y[:, 0], np.round(y_hat[:, 0]))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(recall, precision, color='darkorange',\n",
    "             lw=2, label='PR curve (area = {0}, $F_1 = {1}$'.format(\n",
    "              round(pr_auc, 2), round(f1, 2)))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        plt.annotate('$F_1 = {0:0.1f}$'.format(f_score), xy=(0.87, y[45] + 0.02))\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the PR curve should be around 0.95 and $F_1 \t\\approx 0.87$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_te, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: CNN + RNN (6.874 only - 20 points out of 120 points)\n",
    "\n",
    "(Students enrolled in 6.802, 20.390, 20.490 or HST.506 can also submit a solution for this problem - for an extra 20 points)\n",
    "\n",
    "The binding of some transcription factors depends on the distance-specific existence of co-binding factors. Recall from class that RNNs are good at capturing dependencies between different positions in the input. Thus we are going to explore whether a CNN-RNN hybrid model could achieve a better performance than a regular CNN in classifying motif pairs (NFKB and CTCF in this problem) with variable spacing (10 - 20bp) from the same motif pairs with random spacing in a 101bp sequence. \n",
    "\n",
    "**Note**: \n",
    "- You can use the best CNN hyperparameters obtained from the previous sections for the CNN part of your model.\n",
    "- You should be able to achieve an auROC around 0.85 for the CNN and 0.87 for the CNN + RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_rnn, y_tr_rnn, x_va_rnn, y_va_rnn, x_te_rnn, y_te_rnn = load_data(prefix='rnn_', path='./data/')\n",
    "print(x_tr_rnn.shape, x_va_rnn.shape, x_te_rnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and loss\n",
    "\n",
    "Adapt the `cnn_model` from before by adding a one-layer LSTM RNN (with 48 units) after the max-pooling layer (instead of the fully-connected layer). We will use the softmax of the output of the last LSTM cell as the probability of the two classes. We will ignore regularization and hyperparameter search for this problem (unless you want to explore that).\n",
    "\n",
    "Useful functions:\n",
    "- [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "- [tf.nn.bias_add](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add)\n",
    "- [tf.nn.relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "- [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "- [tf.squeeze](https://www.tensorflow.org/api_docs/python/tf/squeeze)\n",
    "- [tf.transpose](https://www.tensorflow.org/api_docs/python/tf/transpose)\n",
    "- [tf.split](https://www.tensorflow.org/api_docs/python/tf/split)\n",
    "- [tf.nn.rnn_cell.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell)\n",
    "- [tf.nn.static_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn)\n",
    "- [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)\n",
    "- [tf.nn.softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn_model(x_ph):\n",
    "    ################################################################################ \n",
    "    # Implement a CNN-RNN hybrid\n",
    "    # Arguments:\n",
    "    # - x_ph: one-hot encoded DNA input data / placeholder of shape [?, 1, 101, 4]\n",
    "    # Return values:\n",
    "    # - y_hat_op: class probabilities operator (our predicted y, sigmoid(z_op)) of shape [?, 2]\n",
    "    # - z_op: unscaled log probabilities operator (output of last matrix multiplication,\n",
    "    #      before activation function) of shape [?, 2]\n",
    "    ################################################################################\n",
    "    # CNN hyperparameters\n",
    "    conv_filter_height = 1\n",
    "    conv_filter_width = 11\n",
    "    in_channels = 4\n",
    "    out_channels = 32\n",
    "    \n",
    "    max_pool_stride = 2\n",
    "    \n",
    "    # RNN hyperparameters\n",
    "    rnn_units = 48\n",
    "    unroll_length = 51 # complete sequence length (101 after pool stride of 2)\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return y_hat_op, z_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cnn_rnn_loss` is `cnn_loss` without the $L_2$ regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn_loss(z_op, y_ph):\n",
    "    ################################################################################ \n",
    "    # Implement a loss operator, which calculates sigmoid cross entropy loss \n",
    "    # Arguments:\n",
    "    # - z_op: unscaled log probabilities (output of last layer) of shape [?, 2]\n",
    "    # - y_ph: labels / placeholder of shape [?, 2]\n",
    "    # Return values:\n",
    "    # - loss_op: the loss operator\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return loss_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop in `cnn_rnn_training` should be identical to the training loop in `cnn_training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn_training(x_train, y_train, x_val, y_val, num_epochs, batch_size,\n",
    "             save_model=True, model_dir='models/best_model'): \n",
    "    ################################################################################\n",
    "    # Arguments:\n",
    "    # - x_train: input training set\n",
    "    # - y_train: label training set\n",
    "    # - x_val: input validation set\n",
    "    # - y_val: label validation set\n",
    "    # - hyperparam_config: a dictionary that stores a hyperparameter configuration,\n",
    "    #                      including:\n",
    "    #                      - \"dropout_rate\": dropout rate (1 - keep probability),\n",
    "    #                      - \"l2\": coefficient lambda for L2 regularization,\n",
    "    #                      - \"lr\": learning rate for RMSProp optimizer\n",
    "    # - num_epochs: number of epochs to train\n",
    "    # - batch_size: training mini-batch size\n",
    "    # - best_model_dir: location where model will be saved\n",
    "    # Return values:\n",
    "    # - best_loss: best validation loss\n",
    "    ################################################################################\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # define input and output placeholders\n",
    "    # TF uses NHWC: the data is stored in the order of [batch, in_height, in_width, in_channels]\n",
    "    # DNA is one-dimensional (height = 1, width = 101) with four channels for the four bases A, C, G, T\n",
    "    x_ph = tf.placeholder(\"float\", [None, 1, SEQ_LEN, NUM_BASES], name=\"x_ph\")\n",
    "    y_ph = tf.placeholder(\"float\", [None, NUM_CLASSES], name=\"y_ph\")\n",
    "\n",
    "    # model\n",
    "    y_hat_op, z_op = cnn_rnn_model(x_ph)\n",
    "    \n",
    "    # loss\n",
    "    loss_op = cnn_rnn_loss(z_op, y_ph)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(loss_op)\n",
    "     \n",
    "    # Implement training loop\n",
    "    # - loop through training data num_epochs times\n",
    "    # - after each epoch, calculate loss on validation set (x_val, y_val)\n",
    "    # - if save_model: save model with lowest validation loss to disk (model_dir)\n",
    "    # - (delete previously best model)\n",
    "    # - return lowest validation loss\n",
    "    ################################################################################\n",
    "    #                            BEGINNING OF YOUR CODE                            #\n",
    "    ################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    #                               END OF YOUR CODE                               #\n",
    "    ################################################################################\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "\n",
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rnn_model_dir = \"models/best_model_rnn\"\n",
    "validation_loss = cnn_rnn_training(x_tr_rnn, y_tr_rnn, x_va_rnn, y_va_rnn, num_epochs, batch_size, save_model = True,\n",
    "                                   model_dir = best_rnn_model_dir)\n",
    "print('validation loss:', validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the `predict` function from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_rnn = predict(x_te_rnn, best_rnn_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_model_dir = \"models/best_model_cnn\"\n",
    "################################################################################\n",
    "#                            BEGINNING OF YOUR CODE                            #\n",
    "################################################################################\n",
    "# change dropout rate, L2 coefficient, and learning rate to\n",
    "# best values (according to grid search from previous problem):\n",
    "hyperparam_config = {'dropout_rate': 0,\n",
    "                     'l2': 0,\n",
    "                     'lr': 0}\n",
    "################################################################################\n",
    "#                               END OF YOUR CODE                               #\n",
    "################################################################################\n",
    "validation_loss = training(x_tr_rnn, y_tr_rnn, x_va_rnn, y_va_rnn,\n",
    "                           hyperparam_config, num_epochs, batch_size, save_model=True, model_dir=best_cnn_model_dir)\n",
    "print('validation loss:', validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_cnn = predict(x_te_rnn, best_cnn_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y, y_hat, labels):\n",
    "    # RNN\n",
    "    fpr0, tpr0, _ = sklearn.metrics.roc_curve(y[0][:, 0], y_hat[0][:, 0])\n",
    "    roc_auc0 = sklearn.metrics.auc(fpr0, tpr0)\n",
    "    \n",
    "    # CNN\n",
    "    fpr1, tpr1, _ = sklearn.metrics.roc_curve(y[1][:, 0], y_hat[1][:, 0])\n",
    "    roc_auc1 = sklearn.metrics.auc(fpr1, tpr1)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(fpr0, tpr0, color='darkorange',\n",
    "             lw=2, label=labels[0] + ' (area = %0.2f)' % roc_auc0)\n",
    "    plt.plot(fpr1, tpr1, color='blue',\n",
    "             lw=2, label=labels[1] + ' (area = %0.2f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN auROC was around 0.85, and for LSTM around 0.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve([y_te_rnn, y_te_rnn], [y_hat_test_rnn, y_hat_test_cnn], [\"CNN + LSTM\", \"CNN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(y, y_hat, labels):\n",
    "    # RNN\n",
    "    precision0, recall0, _ = sklearn.metrics.precision_recall_curve(y[0][:, 0], y_hat[0][:, 0])\n",
    "    pr_auc0 = sklearn.metrics.auc(recall0, precision0)\n",
    "    f10 = sklearn.metrics.f1_score(y[0][:, 0], np.round(y_hat[0][:, 0]))\n",
    "    \n",
    "    # CNN\n",
    "    precision1, recall1, _ = sklearn.metrics.precision_recall_curve(y[1][:, 0], y_hat[1][:, 0])\n",
    "    pr_auc1 = sklearn.metrics.auc(recall1, precision1)\n",
    "    f11 = sklearn.metrics.f1_score(y[1][:, 0], np.round(y_hat[1][:, 0]))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.plot(recall0, precision0, color='darkorange',\n",
    "             lw=2, label=labels[0] + ' (area = {0}, $F_1 = {1}$)'.format(round(pr_auc0, 2), round(f10, 2)))\n",
    "    plt.plot(recall1, precision1, color='blue',\n",
    "             lw=2, label=labels[1] + ' (area = {0}, $F_1 = {1}$)'.format(round(pr_auc1, 2), round(f11, 2)))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        plt.annotate('$F_1 = {0:0.1f}$'.format(f_score), xy=(0.87, y[45] + 0.02))\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN auPR was around 0.79, and for LSTM around 0.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve([y_te_rnn, y_te_rnn], [y_hat_test_rnn, y_hat_test_cnn], [\"CNN + LSTM\", \"CNN\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
